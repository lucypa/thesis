\chapter{The seL4 Device Driver Framework}\label{ch:sddf}
Unfortunately, many approaches to solving the issues with I/O are limited 
by the monolithic kernel design itself. This presents a strong argument to
redesign a performant yet secure I/O framework on a different architecture completely: the microkernel.
However, the microkernel design has the potential to degrade performance as it requires
significantly more context switches than its monolithic counterpart.
In order to overcome potential performance degradation in I/O systems due to this, we require a simple
framework that minimises system calls when possible for designing performant I/O systems on a microkernel architecture.
This introduces the seL4 Device Driver Framework as an excellent starting point for this project.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{sddf.pdf}
    \caption{The seL4 Device Driver Framework (sDDF) \cite{Parker_22:sddf}}
    \label{f:sddf}
\end{figure}

The seL4 Device Driver Framework (sDDF) aims to rectify any performance degradation of a microkernel
design by providing interfaces and protocols to write performant yet secure user level device
drivers on seL4 \cite{Parker_22:sddf}. It currently supports a statically defined, minimal networking-focused system, and is 
developed on top of the seL4 microkit. The sDDF prioritises a strong separation of 
concerns by componentising each task in an I/O framework such that each component has only a 
single job. This keeps each component inherently very simple, not only minimising any debugging effort required by 
developers but also makes these components ammenable to verification. The sDDF design is based on
top of a simple transport layer that provides protocols for communication between drivers and other
components in the system.

\section{Driver Model}\label{s:driver_model}
The device driver translates a hardware specific device protocol into a hardware independent
(but OS specific) device class protocol. It is inherently event driven, as it only needs to react to
either a client request or an interrupt generated by hardware, and in this way, it maps perfectly onto
the seL4 microkit model. Clients make requests through shared memory and seL4 asynchronous notifications
which simplifies the device driver code to an event loop reacting to either client requests or hardware
events as shown in \autoref{l:driver_pseudo}. 

\begin{figure} [H]
\begin{minted}[]{c}
main() {
    init()
    while(true)
        event = Wait()
        if (event & IRQ)
            handle_irq()
        if (event & CLIENT_REQ)
            handle_request()
}
\end{minted}
\caption{Driver pseudo code}
\label{l:driver_pseudo}
\end{figure}

\section{Transport Layer}
The sDDF transport layer consists of 3 distinct shared memory regions, data structures for data management
and access protocols. These memory regions are:
\begin{enumerate}
    \item \textbf{Metadata region}: the control registers of a device shared between the device itself and its driver. 
    This region is volatile as it is accessed directly by the device and is mapped uncached. 
    \item \textbf{Data region}: buffers containing data that's shared between the device and PDs that require access to the data.
    This region is also accessed directly by the device, but is mapped cached as we assume the device will only access it when 
    instructed. Before and after such accesses, we need to invalidate or clean buffers to ensure cache coherency.
    \item \textbf{Control region}: data structures to manage buffers in the data region. This is shared between two PDs in the system. 
\end{enumerate}

% Something about how we can just update these regions and issue notifications.
\subsection{Control region}
The control region consists of lockless ring buffer queues. These queues are single-producer,
single-consumer (SPSC) which keeps the implementation simple.
The queues keep track of addresses in the data region as shown in \autoref{f:control}. For simplicity,
the figure only shows the control region on the transmit path between a server and driver. Each pair of 
communicating PDs in the sDDF have their own shared control region but share the data region with other PDs. 
The sDDF uses two queues per direction, per control region. For example, as per \autoref{f:control}, the 
Transmit Used (TxU) queue stores buffer addresses which contain data ready for transmit. 
The Transmit Free (TxF) queue stores buffer addresses available for reuse. This is duplicated for the receive path.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{control_region.pdf}
    \caption{Control region between driver and server on transmit path \cite{Parker_22:sddf}}
    \label{f:control}
\end{figure}

The ring buffer queues themselves are very simple. \autoref{l:queues} shows the data structures. Each
ring buffer has a separate head and tail pointer, and entries between the tail and head point to a valid buffer
in the respective data region. Each ring buffer has exactly one producer (the server in \autoref{f:control})
and one consumer (the driver in \autoref{f:control}). The producer
only ever updates the head and the consumer only ever updates the tail.

\begin{figure} [H]
\begin{minted}[]{c}
struct buffer_descr {
    void   *address;
    size_t length;
}
struct ring_buffer {
    uint32_t head;
    uint32_t tail;
    struct buffer_descr buffer[RING_SIZE];
}
\end{minted}
\caption{Ring Buffer Queues}
\label{l:queues}
\end{figure}

Lock free updates to these queues are possible by utilising the property that reads and writes of small
integers are atomic. We simply use a \emph{write memory barrier} before updating the head or tail pointers
as shown in \autoref{l:queues2} to ensure that no reads or writes are re-ordered by the compiler or
processor across this point. As the code ensures there is at least one unused buffer between the head and tail,
the data race is benign and the memory barrier is sufficient to ensure consistency.


\begin{figure} [H]
\begin{minted}[]{c}
bool full(struct ring_buffer *ring)
{
    return (ring->head - ring->tail + 1) % RING_SIZE == 0;
}
bool empty(struct ring_buffer *ring)
{
    return (ring->head - ring->tail) % RING_SIZE == 0;
}
void enqueue(struct buffer_descr *buffer, 
            struct ring_buffer *ring) 
{
    assert ( !full(ring) );
    ring->buffer[ring->head % RING_SIZE] = *buffer;
    barrier();
    ring->head += 1;
}
struct buffer_descr *dequeue(struct ring_buffer *ring)
{
    struct buffer_descr *buffer;
    if (empty(ring)) {
        return NULL;
    } else {
        *buffer = ring->buffer[ring->tail % RING_SIZE];
        barrier();
        ring->tail += 1;
        return *buffer;
    }
}
\end{minted}
\caption{Ring Buffer Queue Management}
\label{l:queues2}
\end{figure}

The queues enable data to be passed between components in batches, thus minimising the number of system calls requried. 

\section{Device Sharing}\label{s:mux_design}
In order to share a device with potentially multiple client applications, the sDDF proposes a simple PD whose 
sole concern is multiplexing the hardware. This multiplexer is implemented at layer one, meaning each client application has its
own virtualised MAC address, and the multiplexer keeps a 1 to 1 mapping of MAC addresses and client CC ids. Multiplexing at layer two
or three would also be possible, but would mean confining applications to a particular protocol or set of ports. 
The multiplexer
can be separated into two separate components. One component handles incoming traffic (Rx Mux), and the other, outgoing traffic
(Tx Mux) as shown in \autoref{f:mux}. Control regions are used by each of these components to communicate with the component on 
either side.
In order to prevent clients from the ability to access each others' data, the shared data region must be split into
separate pools, and a simple copy component per client is responsible for copying data from one memory region to another.
\begin{enumerate}
\item \textbf{Shared Rx data region}: This data region is accessed directly by the device for writing newly received data, as well
as into the Rx Mux's address space and each of the copy components.
\item \textbf{Client Rx data regions}: This data region is mapped into a particular client's address space as well as this
client's copy component. There is a separate client Rx data region per client. The copy component copies data from the 
shared RX data region to the client's own Rx data region.
\item \textbf{Client Tx data regions}: This data region is accessed directly by the device to transmit data, but is also mapped into
the Tx Mux's address space and a particular client's address space. There is one client Tx data region per client. This
enables a zero copy interface on the transmit path.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{mux_design.pdf}
    \caption{Multiple client applications on the sDDF}
    \label{f:mux}
\end{figure}

A multiplexer servicing multiple components inherently requires a policy to determine the order in which it will process work,
however, the sDDF is currently limited to a signle client and as such, does not contain any policy at this stage. Furthermore,
a layer one multiplexer with multiple client applications requires special handling of broadcast protocols. 

\section{Control Flow}
Each component in the sDDF, aside from the client applications, is a single-threaded event-driven program. The
components are simply reacting to an update in either the control or metadata regions, signalled by an seL4 notification. 
To minimise the number of system calls, each component processes as many buffers as it can before signalling the next
relevant component.
The driver runs at the highest priority in the framework. This ensures timely handling of interrupts, as well
as immediate reaction to requests. In order to prevent the driver from monopolising the processor in the case
of high traffic on the receive path, we can limit the size of the receive queue in the metadata region and/or 
the size of the receive queues in the control regions.
The remaining priority ordering is as follows:

\centerline{Driver \(>\) Tx Mux \(>\) Rx Mux \(>\) Clients and per client copiers.}

This priority ordering enables components on the receive path to process as many packets as possible and
thus batch system calls. Due to the bounded size of the queues, only a limited number of packets 
can be processed in one invocation which provides flow control for lower priority components. Note that
client applications may have different priorities. In the case that client A has higher priority
than client B, then it may make sense that client B's copier has lower priority than that of client A.
However, to enable batching, clients should run at lower priority than their copy component. 
On the transmit path, components are invoked as soon as packets are 
ready to be transmitted which keeps transmit latencies low.

\subsection{Current Limitations}
The prototype, as of the start of this thesis, did not support multiple client applications and 
was limited to single core systems. As such, the multiplexer components were very simple and did not contain any policies. Furthermore, the 
design was limited as it expected a client application with symmetric traffic on the receive and transmit paths. In a real
networking system, this would not be the case. For example, a web server would have much higher throughput on the transmit path than the receive path.
Finally, all previous evaluations were limited to running on a single core only which is not the typical set up for high throughput networking systems.
% TODO: Add other stuff that is currently unsupported: eg device discovery, hot plugging, extension for virtual machines although this is out
% of scope for this thesis.

\section{Thesis Problem Statement}
This thesis addresses challenges in developing and evaluating a networking system based on 
the seL4 microkernel and the introduced seL4 Device Driver Framework (sDDF). The overarching 
problem statement encompasses multiple key objectives. Firstly, the aim is to extend and evaluate 
the sDDF framework to provide flexible and secure support for multiple client applications, 
addressing diverse networking demands. Secondly, this thesis focuses on assessing the scalability 
of the framework to multi-core systems, investigating its efficiency across multiple cores to 
optimize performance in a multi-core environment. Additionally, this thesis aims to evaluate the 
framework's resilience against untrusted client applications, examining potential security vulnerabilities 
and proposing a simple solution to protect the subsystem in a given threat scenario from misbehaving clients. 
These objectives collectively contribute to advancing microkernel-based networking systems, offering
a foundation for flexible, scalable, and secure support for multiple client applications while addressing
security concerns associated with untrusted clients. 

