\chapter{Evaluation}\label{ch:evaluation}

We now evaluate and analyse our designs using a series of different benchmarks and running
the system under different configurations. We run all benchmarks on the
Freescale i.MX 8M Mini quad applications processor with 2GB RAM, running at 1.2 GHz
with a 1Gbps NIC. The configurations for each system tested are specified by each benchmark,
and the system comprises of the following PDs:
\begin{enumerate}
    \item Ethernet Driver
    \item (only as specified) Client Side Ethernet Driver
    \item Rx Mux
    \item Tx Mux: implementation specified in each benchmark
    \item ARP Component
    \item Timer Driver
    \item 1 Rx Copy Component per Client application
    \item 1 or more UDP Client applications as specified in each benchmark. 
        The clients use lwIP \cite{Dunkels_01} as an IP stack library for network packet processing. 
    \item (only as specified) Tx Copy Component
\end{enumerate}

For all echo applications, we use ipbench as a distributed load generator 
to send 1472 byte-sized UDP packets to test the seL4 based system. ipbench runs on a 4-node x86 cluster, 
connected to the same switch as our test system,
and counts the number of successful replies from the target system to measure the achieved throughput and latencies.
All benchmarks use a sample size of 200,000 packets unless otherwise specified. 

For some benchmarks, we purposely overload our echo applications. A typical networking system would
perform some computation on incoming data, such as a HTTP web server, which would need to fetch web pages from
memory to process incoming HTTP requests. Although developing such a test scenario is out of scope for this thesis,
we still want to test our framework under high loads where the CPU cannot keep up with the traffic to ascertain the absence
of a performance collapse, as well as ensuring that the system abides by any policy implemented in a multi-client set up. 
We alter our echo application to perform 10 additional data copies and checksum calculations where specified.

We also compare some echo server configurations against a Linux system (version 6.1.1) on the same hardware. 
We use a simple user-level application which reads and then writes all packets immediately back using the socket API. 
In order to determine the CPU utilisation of both systems, we run an idle thread on each core
at low-priority to count the number of idle cycles of that core. 

For all asymmetric client applications, we implement our own custom benchmarking applications to run on an Intel Xeon W-1250
running at 3.3 Ghz, with 64GB RAM and equipped with a 10Gbps NIC. For our transmit-dominant client, our benchmarking
application receives packets using \emph{recv()} and records start and stop time to determine the total transmitted
throughput. For our receive-dominant client, our benchmarking application transmits packets using \emph{send()} and
we count the number of successfully received packets by the client itself to determine the total received throughput.

\section{Single Client Performance}

We first establish a baseline for our system by measuring the networking performance of a single echo server application. 
We also compare against a Linux system and can already establish that our design outperforms a typical monolithic setup 
for simple echo servers. 

\noindent\begin{figure}[H]
    \centering
	\begin{multicols}{2}
		\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{5comp.eps}
        \caption{Linux vs seL4 Networking Performance}
        \label{f:perf}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \vspace{0.15cm}
        \includegraphics[width=1\textwidth]{latency.eps}
        \caption{Linux vs seL4 Networking Latencies}
        \label{f:perf_latencies}
    \end{subfigure}
\end{multicols}
\end{figure}

\autoref{f:perf} compares the performance (achieved throughput and CPU utilisation over applied load) of both systems. 
We can see that the seL4 based system's throughput scales with applied load and manages to saturate the wire, while
Linux scales linearly to about 650 Mb/s and then plateaus. When comparing the CPU utilisation of both systems, we
can see that the reason behind this plateau in throughput is because Linux uses significantly more CPU cycles 
to handle a given load than the seL4 based system. Furthermore, the CPU utilisation of our system also scales 
sub-linearly with increasing load which indicates efficient batching. The batching is a result of the sDDF's asynchronous 
transport layer, which enables data to be passed around in batches and thus minimises the number of system calls requried per packet.

\autoref{f:perf_latencies} compares the round trip time (RTT) latency with standard deviations over applied load of both systems. The latency measured
is the median result taken from a sample size of 200,000 packets. We can see that the sDDF demonstrates a much faster round trip time over Linux.\\

Despite the high count of system calls required in our design, this evaluation demonstrates the minimal impact this count has on overall performance. 
The seL4 based system requires roughly 15K cycles per packet, whereas the Linux based system requires 43K cycles per packet. An asynchronous signal on seL4
is less than 1000 cycles \cite{seL4bench_23}, thus an additional system call per packet would only add an overhead of roughly 6\%. Compared with the cost
of packet processing, this is insignificant, whereas a system call in a monolithic kernel can be of the order of 10s of thousands of cycles. 

\section{Asymmetric Client Applications}

Now that we can support asymmetric traffic in the client application, we implement simple example applications that emulate different
applications that could be deployed on a real system. We wish to test applications with different requirements: 
an application with higher transmit demands and minimal incoming traffic, an application with higher receive demands and minimal outgoing traffic
and finally, an application that initiates communication before transmitting large amounts of data.

\subsection{Transmit-dominant application}\label{s:transmit_dom}
We implement a simple client application that transmits 10,000 UDP packets, of 1472 bytes each, for every UDP packet received.
Ethernet maximum transmission unit is 1518 bytes, which leaves a maximum of 1472 bytes in payload once you account for 14 bytes
in Ethernet headers and 32 bytes for UDP/IP headers. 
In order to interface with lwIP to initiate additional transmits, we first allocate a \texttt{pbuf}, and write 1472 bytes of data
to the \texttt{pbuf}'s payload. As \texttt{pbuf}s are allocated from a memory pool, they can also run out. Should this occur, we temporarily
stop creating transmit requests and store the number of packets we have already sent so we can transmit the rest once more 
memory has freed up. The bottleneck on memory availability for lwIP \texttt{pbuf}s occurs because \texttt{pbuf}s are queued while the application
waits for more transmit buffers as outlined above. Thus, when transmit buffers become available, so to does more memory for
\texttt{pbuf}s. We can thus resume transmitting once the transmit multiplexer has notified the application that more
transmit buffers are available. The benefit of temporarily pausing creating transmit requests is that it also ensures the application
is available to process any incoming traffic in the interim. 

\subsection{Receive-dominant application}
We also implement a simple receive dominant client application that for every 10,000 UDP packets received, it transmits a single one.
This application is much simpler to implement as all memory management is already dealt with so we just adapt the echo server to 
only echo every 10,000th packet.\\

\subsection{Client Initiated Transmit}
Not all networking devices act as servers that only react to incoming traffic. A common use case of a non-reactive application is a 
web client which initiates network communication with another networking device. To test such a scenario, we implement another client
application which first initiates a handshake with another device, then transmits 1,000,000 packets. The initial handshake is there to 
ensure the external benchmarking application is ready to receive all incoming packets. We interface with lwIP as described
in \autoref{s:transmit_dom}.

\subsection{Results}
We now measure the total received throughput by a receive-dominant application (Rx Mostly), the
transmitted throughput by a transmit-dominant application (Tx Mostly) and the transmitted throughput
of a client-initiated transmit application (Tx Initiated) and compare these tests against the achieved
throughput of the echo server. 

\vspace{0.5cm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{asym.eps}
    \caption{Networking performance of different client applications.}
    \label{f:asym}
\end{figure}

\autoref{f:asym} shows the measured throughput as well as the CPU utilisation of each test application. 
Compared with our echo benchmarks, our asymmetric applications both utilise 50\% less CPU for 80+\% of the
throughput. While we would be able to achieve higher throughput in these examples by receiving/sending
more than 10,000 packets for every packet in the other direction, thus skewing the ratio of send/receive in favour of throughput,
this would defeat the purpose of these examples
as we are only wanting to test asymmetric traffic as opposed to uni-directional.
From both of these results, we can conclude our system is able to cope well with asymmetric UDP traffic. 
In our client-initiated benchmark, we are able to achieve near wire speed with less than 40\% CPU. This is inline
with our expectations compared to our echo benchmarks, given this system has half the workload with no incoming
packets. Although these tests are fairly simple, they provide a good sanity check to confirm our system can 
keep up with different workloads and present a good starting point for developing more complex workloads in the future.

\section{Single-core, Multi-client systems}

Our design has a large number of different parameters to experiment with, including multiplexerÆ’
policies, queue sizes and the scheduling parameters of client applications running on the system. 
We select a small variety of example systems to evaluate our different policy designs. We evaluate
these systems with two separate ipbench instances running on different clusters sending UDP packets
at the same time to their respective clients. The order in which they arrive at our system entirely
depends on the order in which packets arrive at the network switch connecting our system to the clusters
running ipbench. 

We start first with evaluating single-core configurations running two echo server applications. 
Each system comprises of an Ethernet driver, Rx Mux, Tx Mux, 2 copy components (1 for client A, 1 for client B) 
and two simple echo servers; client A and client B. As these systems run on single-core, a round-robin or priority-based
policy in the Tx Mux would not make sense as the multiplexer runs at higher priority than both the clients and thus
will be invoked as soon as it is signalled by either client. We instead first evaluate a simple Tx Mux that only 
responds to the client that signalled it, and enforce policy through our system design. We also evaluate our
bandwidth-limited multiplexer on its ability to efficiently enforce different bandwidth limits for different clients.

\subsection{Enforcing Policy through System Design}
We first evaluate a system where both clients run at equal priority, with their respective copy components at 
equal, higher priority than their client, and both clients have a queue size of 512 for all interfacing queues.
We rely on the kernel scheduler to ensure fairness and that clients are scheduled in round-robin order.

\noindent\begin{figure}[H]
    \centering
	\begin{multicols}{2}
		\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{AB.eps}
        \caption{Networking performance of two echo servers at equal priorities.}
        \label{f:AB}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{AB_RxU16.eps}
        \caption{Networking performance of two echo servers at equal priorities, with limited RxU queue of client B.}
        \label{f:AB_Rx16}
    \end{subfigure}
\end{multicols}
\end{figure}

\autoref{f:AB} shows the achieved throughput for client A and B and the CPU utilisation of the entire system. 
Both clients achieve the same throughput, and their combined total reaches wire speed.\\

We now limit the RxU queue between the Rx Mux and the copy component of client B to just 16 to measure 
the impact this will have on client B's throughput. \autoref{f:AB_Rx16} shows the resulting network performance.
Client A still achieves the requested throughput but client B is limited to 400Mbps. This is because the RxU queue
to client B will become full much sooner, causing the Rx Mux to discard subsequent packets for client B and thus
limiting the total throughput available to client B.\\

We now incorporate different priorities to our system and measure the impact this has on each clients
achieved performance. We reduce both client B and its copy component to lower priority than client A. 
Thus the priorities are ordered as follows:\\ 

\begin{minipage}{\textwidth}
\begin{itemize}
    \item[7.] Driver
    \item[6.] Tx Mux
    \item[5.] Rx Mux
    \item[4.] ARP
    \item[4.] Copier of client A
    \item[3.] Client A
    \item[2.] Copier of client B
    \item[1.] Client B
\end{itemize}

\begin{tikzpicture}[overlay]
    \tikzset{my arrow/.style={draw=gray!, single arrow, minimum height=5.5cm, rotate =270, }}
    \node at (6,4) [my arrow] {\rotatebox{360}{Highest to Lowest Priority}};
\end{tikzpicture}
\end{minipage}

We set all queue sizes to be equal. 

\noindent\begin{figure}[H]
    \centering
	\begin{multicols}{2}
		\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{AgtB_RxU32.eps}
        \caption{Throughput achieved by each client.}
        \label{f:AgtB_RxU32_xput}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \vspace{39pt}
        \centering
        \includegraphics[width=1.2\textwidth]{AgtB_RxU32_latency.eps}
        \caption{Median latencies for each client.}
        \label{f:AgtB_RxU32_latency}
    \end{subfigure}
\end{multicols}
\caption{Networking performance of two echo servers at different priorities.}
\label{f:AgtB_RxU32}
\end{figure}

We see in \autoref{f:AgtB_RxU32_xput} the throughput of the lower priority client remains unaffected as there isn't enough
contention on the CPU to limit this client's CPU bandwidth and both clients achieve their requested load.
However, the latencies are affected by this. \autoref{f:AgtB_RxU32_latency} shows the median round trip time (RTT)
for each client. Client A achieves lower latencies than client B as it is running
at higher priority and thus is scheduled to run first.

If there was more contention for the CPU, then we would expect the priority assignment of each client to have an effect
on their achieved throughput. We now add extra overhead to each packet's round trip to simulate an environment where
this is the case. We have each client copy each packet and calculate a checksum 10 times. 

\noindent\begin{figure}[H]
    \centering
	\begin{multicols}{2}
		\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{AgtB_overload.eps}
        \caption{Equal queue sizes.}
        \label{f:AgtB_overload}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{AgtB_RxU32_overload.eps}
        \caption{Limit RxU queue of client B to 32.}
        \label{f:AgtB_RxU32_overload}
    \end{subfigure}
\end{multicols}
\caption{Networking performance of two overloaded echo servers at different priorities.}
\label{f:AgtB_od}
\end{figure}

\autoref{f:AgtB_overload} shows the achieved throughput for both clients. Neither client achieves its requested load, with client A,
the higher priority client, maxing out at 400Mbps and client B seeing a performance collapse to 300Mbps. By the total achieved throughput
and CPU load, it is clear the system is overloaded at this point. However, it is clear that client B as the low-priority client is 
starving client A by monopolising shared Rx buffers, as client A is not able to achieve its requested load. We now limit client B's RxU queue
to 32 to prevent this and ensure there are adequate Rx buffers available to higher priority clients as per \autoref{s:rx_policy}. 
\autoref{f:AgtB_RxU32_overload} shows the resulting performance. The total throughput achieved is not affected by this change, and the
system still becomes overloaded at 400Mbps. However, client A is now able to achieve its requested load as client B's available
Rx throughput is limited. \\

The results all indicate that limiting a client's queues will limit its available throughput. We can use this property to implement 
a bandwidth-limited policy in the system. We now limit the RxU queue between the Mux and Copier of client B to just 16, and also
limit the TxU queue between the Mux and client B to 16. Client B and its Copier still run at lower priority than client A, and both
clients are just simple echo servers (with no additional overload).

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{AgtB_RxTxU16.eps}
    \caption{Networking performance of two echo servers with limited queues to client B.}
    \label{f:AgtB_RxTxU16}
\end{figure}

\autoref{f:AgtB_RxTxU16} shows the resulting performance. Client A still achieves its requested load, but client B is limited
to just 200Mbps. The total CPU load is also reduced compared to the system without any queue limits, indicating less load on 
the limited client.

We now take a closer look at how limiting a clients Tx queues impacts its available throughput. We benchmark a single 
echo client for maximum throughput, and vary the size of both the clients TxU and TxF queues. 

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{queue_size.eps}
    \caption{Impact of queue sizes on networking performance.}
    \label{f:queue_sizes}
\end{figure}

\autoref{f:queue_sizes} shows the throughput achieved and CPU utilisation against the increasing queue sizes. Both 
measurements increase dramatically before plateauing. Notably, there is an efficiency improvement shown in the CPU utilisation
as throughput increases and the queue size doubles from 64 to 128. This is because a limited transmit queue
for an echo application causes incoming packets to be potentially processed by the driver and multiplexer before 
being discarded as the client's RxU queue is full.
This occurs in echo applications due to a shortage of transmit buffers stalling a client's ability to process packets. The
result is that some excess work is done on packets that get discarded, leading to a higher average cycle cost per packet. 
Ultimately, despite this overhead, we see that limiting a client's transmit queues can effectively limit the available transmit
throughput to that client, and with a queue size of 128, a client is able to achieve wire speed.\\

\subsection{Bandwidth-limited Tx Multiplexer}

We can also limit client's available bandwidth using policy in the Tx Mux. We evaluate \autoref{s:bandwidth} with two echo servers,
running at equal priority and with equal queue sizes, and limit client B to just 100Mbps in the Tx Mux. Client A is not limited.

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{Tx_Limited.eps}
    \caption{Networking performance of two echo servers with bandwidth-limited multiplexer.}
    \label{f:Tx_Limited}
\end{figure}

\autoref{f:Tx_Limited} shows the achieved throughput for both clients and the total CPU load of the system. As expected, 
client A which does not have a bandwidth limit, achieves its requested load and client B is limited to just 100Mbps. Furthermore, 
the total CPU load for the total achieved throughput (600Mbps) is only 62\%. Compared to the CPU load of the same 
achieved throughput without any bandwidth limiting (see \autoref{f:AB}) of 52\%, this is less than a 20\% increase. While we
expect there to be some overhead as the multiplexer must communicate regularly with a timer driver, thus incurring additional
context switches, some of this overhead can be reduced with an optimised implementation 
as outlined in \autoref{s:bandwidth}. This is left for future work.

In order to properly evaluate our round-robin and priority-based multiplexer designs, we must first evaluate how our
design scales to multi-core. From there, we can design systems where the clients run on separate cores and will be scheduled irrespective
of one another, thus requiring further policy in the multiplexer.

\section{Multi-core}

Scaling to multi-core will incur overhead from two separate causes. Firstly, seL4 configured with symmetric multi-processing (SMP) increases
the cost of every kernel invocation. This is not only due to the cost of acquiring the kernel lock, but also because the SMP kernel is
less optimised and some fast paths do not apply. Secondly, inter-core communication costs significantly more than intra-core due to the addition of 
inter-processor interrupts (IPI) for inter-core signalling and cache-line bouncing from any shared data. While detailed analysis of
these overheads is left for future work, in order to properly
analyse how our system performs on multi-core configurations, we first establish a baseline of these costs. We first evaluate two separate configurations
using the SMP kernel: pinning every component to a single-core and splitting the load across two cores (out of an available 4). 
We compare these to our baseline without the SMP kernel. Each system is comprised of 5 PDs: an Ethernet driver, RX Mux, 
TX Mux, Copy Component, Client running a simple echo server. Our two core system pins the driver and Tx Mux to one core, and 
the Rx Mux, Copier and Client pinned to the other. We also compare these systems against a typical Linux set up, whereby
the user-space echo server application is pinned to one core, and we allow the Linux scheduler to schedule the remaining 3 cores.\\ 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{multicore.pdf}
    \caption{Networking performance comparison of multi-core systems.}
    \label{f:multicore}
\end{figure}

\autoref{f:multicore} shows the achieved throughput against the CPU utilisation for our 3 different seL4-based systems and Linux. 
As expected, each of our 3 seL4-based systems achieve wire speed (the green and red lines are directly behind the blue line)
whereas the Linux system maxes out at just 800Mbps. All seL4 systems also outperform the Linux system, which utilises more than
1.6 cores for 800Mbps. Further investigation into the Linux system reveals the core running the echo server is maxed out at 
this point, and hence why the system can't achieve higher throughput. \\

However, the total CPU utilisation of each of our seL4 systems differs substantially. 
The difference between the single-core CPU load (dotted green line) and the single-core SMP CPU load (dotted red line) 
outlines the overhead of using the SMP kernel. To achieve 100Mbps, the normal kernel configuration utilises 13.5\% of the CPU, whereas
the SMP kernel configuration utilises 29.9\% of the CPU; about 121\% overhead. While this is substantially higher than expected given
there is no inter-core communication for either of these configurations, detailed
exploration of these overheads is left for future work. 
Comparing the single-core SMP CPU load against the two core SMP CPU load (dotted blue line) outlines the overheads introduced by
inter-core communication between our 5 PDs; just over 20\% at 100Mbps. Note that not all system calls will introduce IPIs in
this configuration, as, for example, the Copy component only communicates intra-core with the Rx Mux and Client. \\

Ultimately, distributing PDs across different cores will incur different overheads. We explore these overheads with another benchmark
comparing 4 separate configurations. Each different configuration isolates one PD on one core, and runs all other PDs on another core. 
We test these configurations with the same echo server benchmark with the same 5 PDs as our previous multi-core benchmark.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{multicore_distr.eps}
    \caption{CPU utilisation with select components isolated on a separate core.}
    \label{f:multicore_distr}
\end{figure}

\autoref{f:multicore_distr} shows the results when isolating the driver, Rx Mux, Copier and Client. The yellow lines show the throughput
and CPU utilisation when the driver is pinned to a separate core. 
Likewise, the red for the Client, the blue for the Rx Mux and green for
the Copier. As expected, each different configuration achieves wire speed but the CPU utilisation of each system differs. Notably, 
the configuration where the driver is isolated incurs the most overhead, followed closely by the configuration isolating the Rx Mux. This is 
expected due to the higher number of system calls per packet each of these PDs makes and thus these systems will incur more overheads from
intra-core signalling and kernel lock contention. Overall, the difference in CPU load of each of these systems is only 4\%, indicating that
components can be arbitrarily distributed across cores. Ultimately, the optimal allocation for a system is use case dependent.\\

Now we have a baseline evaluation for multi-core systems, we can design simple systems with which to evaluate our other transmit multiplexer
designs. 

\subsection{Round-robin Tx Multiplexer}

A round-robin policy in our Tx Multiplexer only makes sense when clients are running on separate cores. This is because the Tx Mux runs at higher
priority than the clients, and will be invoked as soon as a client has signalled it. If both clients are on the same core, then the Tx Mux will
be invoked after one client has finished enqueuing packets for transmit, but before the other client has been scheduled. We evaluate our round-robin
Tx Mux on a 2 client multi-core configuration, where each client is pinned to a separate core shared with its respective copy component, and another
core hosts the Driver, Rx Mux, ARP Component and Tx Mux. 

\noindent\begin{figure}[H]
    \centering
	\begin{multicols}{2}
		\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{Tx_Round_Robin.eps}
        \caption{Throughput achieved and CPU load of two echo servers.}
        \label{f:round_robin_mux}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \vspace{52pt}
        \centering
        \includegraphics[width=\textwidth]{Tx_Round_Robin_latency.eps}
        \vspace{0.5pt}
        \caption{Median latencies of two echo servers.}
        \label{f:round_robin_mux_latency}
    \end{subfigure}
\end{multicols}
\caption{Networking performance of two echo servers with a round-robin multiplexer}
\end{figure}

The total achieved throughput as well as that of each client is shown in \autoref{f:round_robin_mux}. As expected, both clients achieve their 
requested throughput, with their sum equalling wire speed. Importantly, both clients experience the same networking latencies as shown
in \autoref{f:round_robin_mux_latency} by their respective median round trip times. This demonstrates our round-robin Tx Mux enforces
fairness between two clients effectively.\\

The system incurs significant overhead when compared to our equal priority, single-core example in \autoref{f:AB}. To
achieve a total of 200Mbps (100Mbps per client), our multi-core system utilises 63\% of the CPU (split across 3 cores) compared with 
only 23.5\% CPU utilisation of our single-core example. To better understand these overheads, we run our equal priority, 
single-core example again (see \autoref{f:AB}) but with the SMP kernel instead. We find that to achieve
a total throughput of 200Mbps, the system utilises 51\% of the CPU, and the system maxes out at just over 700Mbps. 
From this we can conclude that most of the overheads in these systems stem 
from just using the SMP kernel, and an additional 22\% accounts for the costly intra-core communication across 3 cores as 
well as any overheads of the multiplexer itself. 

\subsection{Priority-based Tx Multiplexer}

We also evaluate our priority-based Tx Mux using the same system design, where we will instead expect a higher priority client to achieve lower
latencies. We set up our priority-based Tx Mux implementation to prioritise client A, followed by client B and finally ARP at lowest priority.
As both clients run on separate cores, their scheduling parameters with respect to one another has no impact and so we allocate them the same
scheduling priority as one another. Likewise with their copy components, which are higher priority than their client and running on the same core. 

\noindent\begin{figure}[h]
    \centering
	\begin{multicols}{2}
		\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{Tx_Priority.eps}
        \caption{Throughput achieved and CPU load of two echo servers.}
        \label{f:priority_mux}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \vspace{52pt}
        \centering
        \includegraphics[width=\textwidth]{Tx_Priority_latency.eps}
        \vspace{0.5pt}
        \caption{Median latencies of two echo servers.}
        \label{f:priority_latency}
    \end{subfigure}
\end{multicols}
\caption{Networking performance of two echo servers with a priority-based multiplexer.}
\end{figure}

Both clients achieve their requested throughput as shown in \autoref{f:priority_mux} and as expected, \autoref{f:priority_latency} shows
that client A has lower latencies over client B. Notably, client A's latencies are also lower on this system, with a median RTT
of 1480us at wire speed, when compared to our system with a round-robin policy, where client A has a median RTT of 1524us at the same
throughput. This demonstrates our policy implemented in the Tx Mux works effectively to prioritise
client A over client B.\\
Furthermore, both our round-robin multiplexer and our priority-based multiplexer introduce similar overheads as indicated by their 
respective CPU utilisation measurements. We conclude from these experiments that our round-robin and priority-based multiplexers work
effectively, with each experiment displaying expected consequences of the policy enforced by the multiplexer, but more work is needed in
investigating the SMP kernel to develop more efficient multi-core systems.\\

\subsection{Two-threaded Driver}
For some multi-core systems, the driver could be a performance bottleneck if the core it is running on is saturated. We can offload
some of the driver's work to another core with our two-threaded driver design. In doing so, we will introduce additional overheads
stemming from an additional context switch per packet, as well as communication between the two driver components required at
high loads. We first measure these overheads by comparing our single-core system with a simple echo server client against the 
same system with our two threaded driver design. We compare the overall throughput and CPU utilisation of both systems. As 
our two-threaded driver design only makes sense in multi-core systems, we configure both systems with SMP enabled but 
initially confine them to a single-core.

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{2driver_comp.eps}
    \caption{Performance comparison of two-threaded driver on multi-core.}
    \label{f:2driver_comp}
\end{figure}

\autoref{f:2driver_comp} shows the results. Both systems achieve the total requested throughput. The CPU utilisation of our 
two-threaded driver (pink dotted line) shows that at low throughputs, the overhead is minimal. However, at high loads, we
see a 6\% increase in CPU load. We hypothesise this overhead is due to the extra notification required between the two 
driver components when the hardware transmit queue becomes full (an unlikely occurrence at low loads).\\

We now design a system where the two-threaded driver design may be applicable. The driver will only be a bottleneck in a
multi-core system when the core it is running on is saturated, and there are no other available cores with sufficient bandwidth
to run the driver on. To replicate such a scenario, we configure our system with 4 compute-heavy client applications. We set up
4 echo servers to copy each packet 100 times and calculate a checksum 100 times. We configure each client to run on its own
core, along with its copy component, and use our round-robin transmit multiplexer to ensure fairness across the clients. 
As our hardware platform only has 4 cores, the driver must share a core with at least one client and 
its copy component. The core assignment is as follows:
\begin{itemize}
    \item[      \textbf{Core 0:}] Driver, Copier of client A, client A
    \item[      \textbf{Core 1:}] Rx Mux, Copier of client B, client B
    \item[      \textbf{Core 2:}] Copier of client C, client C
    \item[      \textbf{Core 3:}] Tx Mux, Copier of client D, client D
\end{itemize}

We test this system with 4 separate ipbench instances, each sending UDP packets to a client and counting received packets
to determine throughput achieved by this client. We request up to 250Mbps for each client for a combined total of 1Gbps on the system.

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{multicore_overload.eps}
    \caption{Networking performance with 4 clients on multi-core.}
    \label{f:multicore_overload}
\end{figure}

\autoref{f:multicore_overload} shows the achieved throughput for each client, as well as the total throughput achieved 
by the system and the total CPU load. Each client is able to achieve 200Mbps but no more. We now configure this system 
with our two-threaded driver, and offload some of the driver's work to core 2 to test whether clients are able to 
achieve higher load. The core assignment is as follows:

\begin{itemize}
    \item[      \textbf{Core 0:}] IRQ-side Driver, Copier of client A, client A
    \item[      \textbf{Core 1:}] Rx Mux, Copier of client B, client B
    \item[      \textbf{Core 2:}] Client-side Driver, Copier of client C, client C
    \item[      \textbf{Core 3:}] Tx Mux, Copier of client D, client D
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{2driver_comp_overload.eps}
    \caption{Performance comparison of two-threaded driver with 4 clients on multi-core.}
    \label{f:2driver_comp_overload}
\end{figure}

\autoref{f:2driver_comp_overload} shows the performance comparison of both systems total throughput achieved and CPU load. 
The system with the two-threaded driver actually achieves slightly lower throughput than the system with the single driver
at the same CPU load. We now take a closer look at the CPU utilisation of each core.

\noindent\begin{figure}[H]
    \centering
	\begin{multicols}{2}
	\begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{core_analysis_multicore_overload.eps}
        \caption{Single driver system.}
        \label{f:core_analysis_mutlicore_overload}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.2\textwidth]{core_analysis_multicore_overload_2driver.eps}
        \caption{Two-threaded driver system.}
        \label{f:core_analysis_mutlicore_overload_2driver}
    \end{subfigure}
\end{multicols}
\caption{Core utilisation of an overloaded 4 client system.}
\label{f:core_util_multi}
\end{figure}

\autoref{f:core_util_multi} shows each core's utilisation against the requested load for both systems. 
In both systems cores 0, 1 and 3 are all saturated. This tells us that although the driver is a potential
bottleneck, both the Rx and Tx multiplexers are too. In \autoref{f:core_analysis_mutlicore_overload_2driver}
we see core 0 has slightly lower utilisation at wire speed compared with
\autoref{f:core_analysis_mutlicore_overload_2driver}, showing that our two-threaded driver design has successfully
offloaded some of the driver load to another core. However, as our multiplexers are also both bottlenecks in 
this example workload, the two-threaded driver design is not enough to improve performance. We hypothesise that
splitting the multiplexers into two separate components, one component responsible for processing used packets and 
the other for processing free packets, may help reduce load on those cores, but such a design is only applicable
on systems with more available CPUs. We leave this exploration for future work.

\section{Transmit Copy Component}

We now measure the overheads introduced by our additional transmit copy component. We implement a new
component between the Tx Mux and our client application as per \autoref{f:tx_copy}. While the implementation
of this component depends on system design and the threat scenario, we measure the total possible overhead
added if the component is responsible for all possible tasks listed. These are:
\begin{itemize}
    \item Ensuring metadata enqueued to the multiplexer is sane.
    \item Checking outgoing packets are well formed 
    \item Copying outgoing packets into a separate region not mapped
          into the client's address space.
    \item Interfacing with the multiplexer correctly by incrementing the head/tail pointers as required.
    \item Ignoring any transmit packets from the client that do not abide by the protocols.
\end{itemize}

We measure these overheads on a simple echo server system. We run this new component at higher priority than the client
and the Rx Copy component to ensure invocation as soon as packets are ready to be transmitted and thus keep
latencies low. We use a simple Tx Mux configured for a single client and the priority assignment of the system is as follows:\\
\centerline{Driver \(>\) Tx Mux  \(>\) Tx Copy \(>\) Rx Mux \(>\) Rx Copy \(>\) Client}

We compare the achieved throughput and CPU utilisation of this system to the same system without this additional component. 

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{tx_copy_overhead.eps}
    \caption{Performance overhead with Tx Copy component.}
    \label{f:tx_copy_overhead}
\end{figure}

\autoref{f:tx_copy_overhead} shows the resulting performance with and without the Tx Copy component. As expected, both
systems achieve wire speed. At 100Mbps, our system utilises 16\% CPU with our Tx Copy component, compared to 13.5\% CPU 
without the additional component. This is about an 18\% overhead. To achieve 958Mbps, our system utilises 97\% CPU with our Tx
Copy component, compared with 82.5\% CPU utilisation without it, thus showing a 17\% increase. This overhead is 
inline with our expectations as we are comparing systems with 6 and 5 components respectively, thus a 20\% increase
in component count alone. Of course, this overhead can be reduced depending on the threat model, as some systems might
rely on the NIC to ensure packets are well formed, or use the IOMMU to sanitise all buffer addresses.  

\section{Summary}

Overall, our single client echo benchmarks outperform Linux significantly, both in single and multi-core set ups.
This demonstrates a highly componentised, microkernel-based architecture is a feasible design despite the extra number
of context switches required. Our exploration into asymmetric traffic demonstrates our framework 
can handle such loads efficiently.\\
Our multi-client results indicate the framework design, along with our new multiplexer 
designs can flexibly implement a variety of different policies that may be required depending on system use-case.
However, not all combinations make sense and system design heavily influences inner policies. 
For example, \autoref{f:AgtB_overload} demonstrated
that a higher priority client could be starved by a lower priority client without appropriate choice
of queue size, and simply limiting a clients queue sizes can reduce a clients available throughput. System designers
can use these properties to implement a use-case specific policy \\

Although our design scales well to multiple clients, our investigation into multi-core systems has 
unveiled significant overheads from just enabling the SMP configuration in seL4.
Thorough investigation of these overheads is left for future work. In spite of such overheads, our design can
be arbitrarily distributed across cores and effectively enables high-throughput networking for compute-heavy clients when a 
single CPU is insufficient. From our analysis, we conclude our two-threaded driver design is not beneficial
due to the overheads outweighing any possible benefit in systems where the driver is a bottleneck in throughput.\\

Finally, although a security analysis of our framework reveals a misbehaving client does have the ability to compromise
the entire transmit path for all clients, these vulnerabilities can be easily eliminated with an additional component between 
the client and the shared multiplexer. This component introduces acceptable overheads for the stronger security
guarantees provided, but can also be easily configured for the systems threat scenario and performance requirements. 
